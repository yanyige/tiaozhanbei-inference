import logging
import os
import csv
import json
import time
from datetime import datetime
from pathlib import Path
from config import LOG_CONFIG, DEFAULT_MODEL_PATH, LOGS_BASE_DIR, STATS_FILE_PATH


class PerformanceLogger:
    def __init__(self, model_path=None):
        # å¦‚æœæ²¡æœ‰æŒ‡å®šæ¨¡å‹è·¯å¾„ï¼Œä½¿ç”¨é…ç½®æ–‡ä»¶ä¸­çš„é»˜è®¤å€¼
        if model_path is None:
            model_path = DEFAULT_MODEL_PATH
            
        self.model_path = model_path
        # æå–æ¨¡å‹åç§°
        self.model_name = self._extract_model_name(model_path)
        
        # ç”Ÿæˆåºå·å’Œæ—¶é—´æˆ³
        self.run_id = self._get_next_run_id()
        self.timestamp = datetime.now().strftime("%Y%m%d-%H%M%S")
        
        # åˆ›å»ºç›®å½•ç»“æ„
        self._setup_directories()
        
        # è®¾ç½®æ–‡ä»¶è·¯å¾„
        self._setup_file_paths()
        
        # é…ç½®æ—¥å¿—ç³»ç»Ÿ
        self._setup_logging()
        
        # æ€§èƒ½ç»Ÿè®¡æ•°æ®
        self.stats = {
            'batch_times': {},
            'start_time': None
        }
        
        # è®°å½•å¼€å§‹ä¿¡æ¯
        self.logger.info(f"åˆå§‹åŒ–æ€§èƒ½è®°å½•å™¨ - æ¨¡å‹: {self.model_name}, è¿è¡Œåºå·: {self.run_id}")

    # ä»æ¨¡å‹è·¯å¾„æå–æ¨¡å‹åç§°
    def _extract_model_name(self, model_path):
        # ä¾‹å¦‚: "/home/ma-user/work/Qwen2.5-7B-Instruct" -> "Qwen2.5-7B-Instruct"
        model_name = Path(model_path).name
        # æ¸…ç†æ–‡ä»¶åä¸­çš„ç‰¹æ®Šå­—ç¬¦
        model_name = model_name.replace("/", "_").replace("\\", "_")
        return model_name

    # è·å–ä¸‹ä¸€ä¸ªè¿è¡Œåºå·ï¼ˆå…¨å±€è‡ªå¢ï¼Œè·¨æ¨¡å‹ï¼‰
    def _get_next_run_id(self):
        # ä»performance_stats.csvæ–‡ä»¶ä¸­è¯»å–å½“å‰æœ€å¤§ID
        current_max_id = 0
        
        # ç¡®ä¿æ—¥å¿—ç›®å½•å­˜åœ¨
        os.makedirs(LOGS_BASE_DIR, exist_ok=True)
        
        # éœ€è¦æ£€æŸ¥çš„æ–‡ä»¶åˆ—è¡¨ï¼ˆåŒ…æ‹¬å¤‡ä»½æ–‡ä»¶ï¼‰
        files_to_check = [STATS_FILE_PATH]
        backup_file = STATS_FILE_PATH + '.backup'
        if os.path.exists(backup_file):
            files_to_check.append(backup_file)
        
        for file_path in files_to_check:
            if os.path.exists(file_path):
                try:
                    with open(file_path, 'r', encoding='utf-8') as f:
                        reader = csv.DictReader(f)
                        for row in reader:
                            try:
                                run_id = int(row.get('run_id', '0'))
                                current_max_id = max(current_max_id, run_id)
                            except (ValueError, TypeError):
                                # è·³è¿‡æ— æ•ˆçš„run_id
                                continue
                except (IOError, csv.Error):
                    # å¦‚æœæ–‡ä»¶è¯»å–å¤±è´¥ï¼Œç»§ç»­å¤„ç†ä¸‹ä¸€ä¸ªæ–‡ä»¶
                    continue
        
        # ç”Ÿæˆæ–°çš„ID
        new_id = current_max_id + 1
        
        return f"{new_id:03d}"

    # åˆ›å»ºç›®å½•ç»“æ„
    def _setup_directories(self):
        # ä½¿ç”¨é…ç½®çš„æ—¥å¿—ç›®å½•
        self.logs_base_dir = LOGS_BASE_DIR
        
        # æ¨¡å‹ç‰¹å®šç›®å½•
        self.model_logs_dir = os.path.join(self.logs_base_dir, self.model_name)
        
        # åˆ›å»ºç›®å½•
        os.makedirs(self.model_logs_dir, exist_ok=True)

    # è®¾ç½®æ–‡ä»¶è·¯å¾„
    def _setup_file_paths(self):
        # ç»“æœæ–‡ä»¶: logs/Qwen2.5-7B-Instruct/001-Qwen2.5-7B-Instruct-20241219-143022.json
        self.output_filename = f"{self.run_id}-{self.model_name}-{self.timestamp}.json"
        self.output_path = os.path.join(self.model_logs_dir, self.output_filename)
        
        # ç»Ÿè®¡æ–‡ä»¶: ä½¿ç”¨é…ç½®çš„ç»Ÿè®¡æ–‡ä»¶è·¯å¾„
        self.stats_file = STATS_FILE_PATH
        
        # æ—¥å¿—æ–‡ä»¶: logs/Qwen2.5-7B-Instruct/001-Qwen2.5-7B-Instruct-20241219-143022.log
        self.log_filename = f"{self.run_id}-{self.model_name}-{self.timestamp}.log"
        self.log_file = os.path.join(self.model_logs_dir, self.log_filename)

    # é…ç½®æ—¥å¿—ç³»ç»Ÿ
    def _setup_logging(self):
        # åˆ›å»ºlogger
        self.logger = logging.getLogger(f"performance_logger_{self.run_id}")
        
        # è®¾ç½®æ—¥å¿—çº§åˆ«
        log_level = getattr(logging, LOG_CONFIG["level"].upper())
        self.logger.setLevel(log_level)
        
        # æ¸…é™¤ç°æœ‰çš„handlers
        self.logger.handlers.clear()
        
        # æ–‡ä»¶handler - è¯¦ç»†æ—¥å¿—
        file_handler = logging.FileHandler(self.log_file, encoding=LOG_CONFIG["encoding"])
        file_handler.setLevel(log_level)
        
        # æ§åˆ¶å°handler - ç®€åŒ–è¾“å‡º
        console_handler = logging.StreamHandler()
        console_handler.setLevel(log_level)
        
        # æ ¼å¼å™¨
        detailed_formatter = logging.Formatter(
            '%(asctime)s - %(name)s - %(levelname)s - %(message)s',
            datefmt='%Y-%m-%d %H:%M:%S'
        )
        simple_formatter = logging.Formatter('%(message)s')
        
        file_handler.setFormatter(detailed_formatter)
        console_handler.setFormatter(simple_formatter)
        
        # æ·»åŠ handlers
        self.logger.addHandler(file_handler)
        self.logger.addHandler(console_handler)

    # å¼€å§‹è®¡æ—¶
    def start_timing(self):
        self.stats['start_time'] = time.time()
        self.logger.info("å¼€å§‹å¤„ç†...")

    # è®°å½•æ‰¹æ¬¡å¼€å§‹
    def log_batch_start(self, question_type, count):
        self.logger.info(f"å¼€å§‹å¤„ç† {question_type} ç±»å‹é¢˜ç›®ï¼Œå…± {count} é“...")
        return time.time()

    # è®°å½•æ‰¹æ¬¡ç»“æŸ
    def log_batch_end(self, question_type, count, batch_start_time, generation_time):
        batch_total_time = time.time() - batch_start_time
        avg_time = batch_total_time / count if count > 0 else 0
        
        # ä¿å­˜ç»Ÿè®¡æ•°æ®
        self.stats['batch_times'][question_type] = {
            'count': count,
            'total_time': batch_total_time,
            'avg_time': avg_time,
            'generation_time': generation_time
        }
        
        self.logger.info(
            f"å®Œæˆ {question_type} ç±»å‹ï¼Œ{count} é“é¢˜ï¼Œ"
            f"æ€»è€—æ—¶ {batch_total_time:.2f}sï¼Œå¹³å‡ {avg_time:.2f}s/é¢˜"
        )

    # è®°å½•å•ä¸ªé—®é¢˜å®Œæˆ
    def log_question_complete(self, question_id, question_type, has_multiple_answers=False):
        if has_multiple_answers:
            self.logger.debug(f"å®Œæˆé¢˜ç›® {question_id} ({question_type}, å¤šå€™é€‰ç­”æ¡ˆ)")
        else:
            self.logger.debug(f"å®Œæˆé¢˜ç›® {question_id} ({question_type})")

    # ä¿å­˜ç»“æœæ–‡ä»¶
    def save_results(self, results):
        final_result = {"result": {"results": results}}
        
        with open(self.output_path, "w", encoding="utf-8") as f:
            json.dump(final_result, f, ensure_ascii=False, indent=2)
        
        self.logger.info(f"ç»“æœå·²ä¿å­˜åˆ°: {self.output_path}")
    
    # å¢é‡ä¿å­˜å•ä¸ªç»“æœåˆ°JSONæ–‡ä»¶
    def save_single_result(self, new_result, all_results):
        # å°†æ–°ç»“æœæ·»åŠ åˆ°æ€»ç»“æœåˆ—è¡¨
        all_results.append(new_result)
        
        # ç«‹å³ä¿å­˜åˆ°æ–‡ä»¶
        final_result = {"result": {"results": all_results}}
        
        with open(self.output_path, "w", encoding="utf-8") as f:
            json.dump(final_result, f, ensure_ascii=False, indent=2)
        
        print(f"    ğŸ’¾ å·²ä¿å­˜é¢˜ç›® {new_result['id']} çš„ç»“æœ ({len(all_results)} é¢˜æ€»è®¡)")
        self.logger.debug(f"ğŸ’¾ å·²ä¿å­˜é¢˜ç›® {new_result['id']} çš„ç»“æœ ({len(all_results)} é¢˜æ€»è®¡)")
        
        return all_results

    # è®°å½•æ€§èƒ½ç»Ÿè®¡æ€»ç»“
    def log_performance_summary(self, total_questions):
        if self.stats['start_time'] is None:
            self.logger.error("æœªè°ƒç”¨ start_timing()ï¼Œæ— æ³•è®¡ç®—æ€»æ—¶é—´")
            return
            
        total_time = time.time() - self.stats['start_time']
        
        # è®°å½•åˆ°CSVæ–‡ä»¶
        self._save_performance_csv(total_questions, total_time)
        
        # è®°å½•è¯¦ç»†ç»Ÿè®¡
        self.logger.info("="*50)
        self.logger.info("æ€§èƒ½ç»Ÿè®¡æ€»ç»“:")
        self.logger.info(f"  è¿è¡Œåºå·: {self.run_id}")
        self.logger.info(f"  æ¨¡å‹åç§°: {self.model_name}")
        self.logger.info(f"  æ€»é¢˜æ•°: {total_questions}")
        self.logger.info(f"  æ€»è€—æ—¶: {total_time:.2f}s")
        self.logger.info(f"  å¹³å‡è€—æ—¶: {total_time/total_questions:.2f}s/é¢˜" if total_questions > 0 else "  å¹³å‡è€—æ—¶: N/A")
        
        for qtype, stats in self.stats['batch_times'].items():
            self.logger.info(f"  {qtype}: {stats['count']}é¢˜, å¹³å‡{stats['avg_time']:.2f}s/é¢˜")
        
        self.logger.info(f"  ç»Ÿè®¡æ–‡ä»¶: {self.stats_file}")
        self.logger.info(f"  æ—¥å¿—æ–‡ä»¶: {self.log_file}")
        self.logger.info("="*50)

    # ä¿å­˜æ€§èƒ½ç»Ÿè®¡åˆ°CSV
    def _save_performance_csv(self, total_questions, total_time):
        # å‡†å¤‡CSVæ•°æ®è¡Œ
        row_data = {
            'run_id': self.run_id,
            'model_name': self.model_name,
            'timestamp': self.timestamp,
            'datetime': datetime.now().strftime('%Y-%m-%d %H:%M:%S'),
            'total_questions': total_questions,
            'total_time': round(total_time, 2),
            'avg_time_per_question': round(total_time / total_questions, 2) if total_questions > 0 else 0,
            'output_file': self.output_filename,
            'log_file': self.log_filename
        }
        
        # æ·»åŠ å„é¢˜å‹çš„ç»Ÿè®¡æ•°æ®
        all_types = ['choice', 'code-generate', 'generic-generate', 'math']
        for qtype in all_types:
            if qtype in self.stats['batch_times']:
                stats = self.stats['batch_times'][qtype]
                row_data[f'{qtype}_count'] = stats['count']
                row_data[f'{qtype}_total_time'] = round(stats['total_time'], 2)
                row_data[f'{qtype}_avg_time'] = round(stats['avg_time'], 2)
                row_data[f'{qtype}_generation_time'] = round(stats['generation_time'], 2)
            else:
                row_data[f'{qtype}_count'] = 0
                row_data[f'{qtype}_total_time'] = 0
                row_data[f'{qtype}_avg_time'] = 0
                row_data[f'{qtype}_generation_time'] = 0
        
        # å®šä¹‰CSVè¡¨å¤´
        fieldnames = [
            'run_id', 'model_name', 'timestamp', 'datetime', 
            'total_questions', 'total_time', 'avg_time_per_question',
            'output_file', 'log_file'
        ]
        for qtype in all_types:
            fieldnames.extend([
                f'{qtype}_count', f'{qtype}_total_time', 
                f'{qtype}_avg_time', f'{qtype}_generation_time'
            ])
        
        # æ£€æŸ¥æ–‡ä»¶æ˜¯å¦å­˜åœ¨ä»¥åŠè¡¨å¤´æ˜¯å¦ä¸€è‡´
        file_exists = os.path.exists(self.stats_file)
        need_header = True
        
        if file_exists:
            # æ£€æŸ¥ç°æœ‰æ–‡ä»¶çš„è¡¨å¤´æ˜¯å¦ä¸å½“å‰fieldnamesä¸€è‡´
            try:
                with open(self.stats_file, 'r', encoding='utf-8') as f:
                    existing_reader = csv.DictReader(f)
                    existing_fieldnames = existing_reader.fieldnames
                    if existing_fieldnames == fieldnames:
                        need_header = False
                    else:
                        # è¡¨å¤´ä¸ä¸€è‡´ï¼Œå¤‡ä»½æ—§æ–‡ä»¶å¹¶é‡æ–°åˆ›å»º
                        backup_file = self.stats_file + '.backup'
                        os.rename(self.stats_file, backup_file)
                        self.logger.info(f"è¡¨å¤´ä¸ä¸€è‡´ï¼Œå·²å¤‡ä»½æ—§æ–‡ä»¶åˆ°: {backup_file}")
                        file_exists = False
            except (IOError, csv.Error):
                # æ–‡ä»¶æŸåæˆ–æ ¼å¼é”™è¯¯ï¼Œé‡æ–°åˆ›å»º
                if os.path.exists(self.stats_file):
                    backup_file = self.stats_file + '.backup'
                    os.rename(self.stats_file, backup_file)
                    self.logger.info(f"æ–‡ä»¶æ ¼å¼é”™è¯¯ï¼Œå·²å¤‡ä»½æ—§æ–‡ä»¶åˆ°: {backup_file}")
                file_exists = False
        
        # å†™å…¥CSVæ–‡ä»¶
        mode = 'a' if file_exists else 'w'
        with open(self.stats_file, mode, newline='', encoding='utf-8') as f:
            writer = csv.DictWriter(f, fieldnames=fieldnames)
            
            # å¦‚æœéœ€è¦å†™å…¥è¡¨å¤´
            if need_header:
                writer.writeheader()
                if not file_exists:
                    self.logger.info(f"åˆ›å»ºæ€§èƒ½ç»Ÿè®¡æ–‡ä»¶: {self.stats_file}")
                else:
                    self.logger.info(f"æ›´æ–°æ€§èƒ½ç»Ÿè®¡æ–‡ä»¶è¡¨å¤´: {self.stats_file}")
            
            # å†™å…¥æ•°æ®è¡Œ
            writer.writerow(row_data)

    # è·å–æœ€æ–°çš„ç»“æœæ–‡ä»¶è·¯å¾„ï¼Œç”¨äºæ–­ç‚¹ç»­ä¼ 
    def get_latest_result_file(self):
        if not os.path.exists(self.model_logs_dir):
            return None
            
        result_files = [f for f in os.listdir(self.model_logs_dir) 
                       if f.endswith('.json') and f != self.output_filename]
        
        if not result_files:
            return None
        
        # é€‰æ‹©æœ€æ–°çš„ç»“æœæ–‡ä»¶
        latest_file = max(result_files, 
                         key=lambda x: os.path.getctime(os.path.join(self.model_logs_dir, x)))
        
        return os.path.join(self.model_logs_dir, latest_file)

    # è®°å½•æ–­ç‚¹ç»­ä¼ ä¿¡æ¯
    def log_recovery(self, recovery_file, recovered_count):
        self.logger.info(f"ä» {recovery_file} æ¢å¤å†å²è¿›åº¦ï¼Œå·²å¤„ç† {recovered_count} é¢˜")

    # è®°å½•é”™è¯¯ä¿¡æ¯
    def log_error(self, error_message):
        self.logger.error(error_message)

    # å…³é—­æ—¥å¿—å™¨
    def close(self):
        # æ¸…ç†handlers
        for handler in self.logger.handlers[:]:
            handler.close()
            self.logger.removeHandler(handler) 