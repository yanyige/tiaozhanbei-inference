import json
import re
import os
import time
import argparse
import torch

# è®¾ç½®ç¯å¢ƒå˜é‡å¼ºåˆ¶ç¦»çº¿æ¨¡å¼ï¼Œé¿å…è¿æ¥ HuggingFace Hub
os.environ['HF_HUB_OFFLINE'] = '1'
os.environ['TRANSFORMERS_OFFLINE'] = '1'
os.environ['HF_DATASETS_OFFLINE'] = '1'

# å¯¼å…¥åä¸ºæ˜‡è…¾NPUæ”¯æŒ
try:
    import torch_npu
    print("åä¸ºæ˜‡è…¾NPUæ”¯æŒå·²åŠ è½½")
except ImportError:
    print("è­¦å‘Š: æœªæ‰¾åˆ°torch_npuï¼Œå°†ä½¿ç”¨CPUæˆ–GPU")

from transformers import AutoModelForCausalLM, AutoTokenizer
from logger import PerformanceLogger
from config import get_model_path, DEFAULT_DATA_PATH, DEFAULT_GENERATION_CONFIG, list_available_models, needs_trust_remote_code, clean_log_files

class Main:
    def __init__(self, model_path, data_path, device="auto", test_mode=False):
        self.model_path = model_path
        self.data_path = data_path
        self.test_mode = test_mode
        
        # è®¾å¤‡æ£€æµ‹å’Œé…ç½®
        self.device = self._get_device(device)
        print(f"ğŸ”§ ä½¿ç”¨è®¾å¤‡: {self.device}")
        
        # åˆ›å»ºæ¨¡å‹å’Œtokenizerå®ä¾‹ï¼Œæ ¹æ®é…ç½®å†³å®šæ˜¯å¦ä½¿ç”¨trust_remote_code
        trust_remote_code = needs_trust_remote_code(model_path)
        if trust_remote_code:
            print(f"æ¨¡å‹ {model_path} éœ€è¦trust_remote_code=True")
        
        try:
            self.tokenizer = AutoTokenizer.from_pretrained(model_path, trust_remote_code=True)
            
            # æ ¹æ®è®¾å¤‡ç±»å‹é…ç½®æ¨¡å‹åŠ è½½å‚æ•°
            model_kwargs = {
                "trust_remote_code": True,
            }
            
            # å¯¹äºNPUï¼Œä½¿ç”¨float32å¯èƒ½æ›´ç¨³å®š
            if self.device.startswith("npu"):
                model_kwargs["torch_dtype"] = torch.float32
                print("ğŸ”§ NPUè®¾å¤‡ä½¿ç”¨float32ç²¾åº¦")
            else:
                model_kwargs["torch_dtype"] = torch.float16  # å…¶ä»–è®¾å¤‡ä½¿ç”¨åŠç²¾åº¦
                print("ğŸ”§ ä½¿ç”¨float16åŠç²¾åº¦ä»¥èŠ‚çœå†…å­˜")
            
            self.model = AutoModelForCausalLM.from_pretrained(model_path, **model_kwargs)
            
            # æ‰‹åŠ¨ç§»åŠ¨æ¨¡å‹åˆ°æŒ‡å®šè®¾å¤‡
            print(f"ğŸ“¦ æ­£åœ¨å°†æ¨¡å‹ç§»åŠ¨åˆ°è®¾å¤‡: {self.device}")
            self.model = self.model.to(self.device)
            
            # ç¡®ä¿æœ‰pad_token_id
            if self.tokenizer.pad_token_id is None:
                self.tokenizer.pad_token_id = self.tokenizer.eos_token_id
            
            print("æ¨¡å‹åŠ è½½æˆåŠŸï¼")
        except Exception as e:
            print(f"åŠ è½½æ¨¡å‹å¤±è´¥: {e}")
            if trust_remote_code:
                print(f"æ¨¡å‹ {model_path} éœ€è¦ trust_remote_code=Trueï¼Œè¯·ç¡®ä¿æ¨¡å‹æ–‡ä»¶å®Œæ•´ä¸”æ”¯æŒç¦»çº¿åŠ è½½")
            raise
        
        # åˆå§‹åŒ–æ€§èƒ½æ—¥å¿—å™¨
        self.logger = PerformanceLogger(model_path=model_path)
        
        # åˆå§‹åŒ–æç¤ºæ¨¡æ¿
        self.prompt_templates = {
            "choice": (
                "Answer the following multiple choice question. The last line of your response should be of the following format: "
                "'Answer: $LETTER' (without quotes) where LETTER is one of ABCD. Think step by step before answering.\n\n"
                "{Question}\n\nA) {A}\nB) {B}\nC) {C}\nD) {D}\n"
                "è¯·ä»¥<think>æ¨ç†è¿‡ç¨‹</think><answer>æœ€ç»ˆç­”æ¡ˆ</answer>çš„æ ¼å¼è¾“å‡ºã€‚"
            ),
            "code-generate": (
                "Read the following function signature and docstring, and fully implement the function described. Your response should only contain the code for this function.\n"
                "{Question}\n"
                "è¯·ä»¥<think>æ¨ç†è¿‡ç¨‹</think><answer>æœ€ç»ˆä»£ç </answer>çš„æ ¼å¼è¾“å‡ºã€‚"
            ),
            "generic-generate": (
                "You will be asked to read a passage and answer a question. Think step by step, then write a line of the form 'Answer: $ANSWER' at the end of your response.\n"
                "{Question}\n"
                "è¯·ä»¥<think>æ¨ç†è¿‡ç¨‹</think><answer>æœ€ç»ˆç­”æ¡ˆ</answer>çš„æ ¼å¼è¾“å‡ºã€‚"
            ),
            "math": (
                "Solve the following math problem step by step. The last line of your response should be of the form Answer: $ANSWER (without quotes) where $ANSWER is the answer to the problem.\n\n"
                "{Question}\n\n"
                "Remember to put your answer on its own line after 'Answer:', and indicate your final answer in boxed LaTeX. For example, if the final answer is \\sqrt{{3}}, write it as \\boxed{{\\sqrt{{3}}}}ã€‚\n"
                "è¯·ä»¥<think>æ¨ç†è¿‡ç¨‹</think><answer>æœ€ç»ˆç­”æ¡ˆ</answer>çš„æ ¼å¼è¾“å‡ºã€‚"
            ),
        }
        
        # é‡‡æ ·å‚æ•°è®¾ç½®ï¼ˆä¸ºç¨³å®šæ€§ä¼˜åŒ–ï¼‰
        self.generation_params = {
            "choice": {
                "max_new_tokens": min(DEFAULT_GENERATION_CONFIG["max_tokens"]["choice"], 512), 
                "temperature": DEFAULT_GENERATION_CONFIG["temperature"], 
                "top_p": DEFAULT_GENERATION_CONFIG["top_p"],
                "do_sample": True,
                "pad_token_id": self.tokenizer.eos_token_id,
                "repetition_penalty": 1.1  # é¿å…é‡å¤ç”Ÿæˆ
            },
            "code-generate": {
                "max_new_tokens": min(DEFAULT_GENERATION_CONFIG["max_tokens"]["code-generate"], 1024), 
                "temperature": DEFAULT_GENERATION_CONFIG["temperature"], 
                "top_p": DEFAULT_GENERATION_CONFIG["top_p"],
                "do_sample": True,
                "pad_token_id": self.tokenizer.eos_token_id,
                "num_return_sequences": 3,
                "repetition_penalty": 1.1
            },
            "generic-generate": {
                "max_new_tokens": min(DEFAULT_GENERATION_CONFIG["max_tokens"]["generic-generate"], 512), 
                "temperature": DEFAULT_GENERATION_CONFIG["temperature"], 
                "top_p": DEFAULT_GENERATION_CONFIG["top_p"],
                "do_sample": True,
                "pad_token_id": self.tokenizer.eos_token_id,
                "repetition_penalty": 1.1
            },
            "math": {
                "max_new_tokens": min(DEFAULT_GENERATION_CONFIG["max_tokens"]["math"], 512), 
                "temperature": DEFAULT_GENERATION_CONFIG["temperature"], 
                "top_p": DEFAULT_GENERATION_CONFIG["top_p"],
                "do_sample": True,
                "pad_token_id": self.tokenizer.eos_token_id,
                "repetition_penalty": 1.1
            }
        }
    
    # æ£€æµ‹å’Œé…ç½®å¯ç”¨çš„è®¾å¤‡
    def _get_device(self, device_preference="auto"):
        if device_preference == "auto":
            # è‡ªåŠ¨æ£€æµ‹è®¾å¤‡
            # ä¼˜å…ˆæ£€æµ‹NPU
            if hasattr(torch, 'npu') and torch.npu.is_available():
                device_count = torch.npu.device_count()
                print(f"ğŸš€ è‡ªåŠ¨æ£€æµ‹åˆ° {device_count} ä¸ªNPUè®¾å¤‡")
                return f"npu:0"  # ä½¿ç”¨ç¬¬ä¸€ä¸ªNPUè®¾å¤‡
            
            # æ£€æµ‹GPU
            elif torch.cuda.is_available():
                device_count = torch.cuda.device_count()
                print(f"ğŸš€ è‡ªåŠ¨æ£€æµ‹åˆ° {device_count} ä¸ªGPUè®¾å¤‡")
                return "cuda:0"  # ä½¿ç”¨ç¬¬ä¸€ä¸ªGPUè®¾å¤‡
            
            # ä½¿ç”¨CPU
            else:
                print("âš ï¸  æœªæ£€æµ‹åˆ°NPUæˆ–GPUï¼Œå°†ä½¿ç”¨CPU")
                return "cpu"
        else:
            # ç”¨æˆ·æŒ‡å®šè®¾å¤‡
            if device_preference == "npu":
                if hasattr(torch, 'npu') and torch.npu.is_available():
                    print(f"ğŸš€ ä½¿ç”¨ç”¨æˆ·æŒ‡å®šçš„NPUè®¾å¤‡")
                    return "npu:0"
                else:
                    print("âŒ NPUä¸å¯ç”¨ï¼Œå›é€€åˆ°CPU")
                    return "cpu"
            elif device_preference == "cuda":
                if torch.cuda.is_available():
                    print(f"ğŸš€ ä½¿ç”¨ç”¨æˆ·æŒ‡å®šçš„GPUè®¾å¤‡")
                    return "cuda:0"
                else:
                    print("âŒ GPUä¸å¯ç”¨ï¼Œå›é€€åˆ°CPU")
                    return "cpu"
            else:
                print(f"ğŸ”§ ä½¿ç”¨ç”¨æˆ·æŒ‡å®šçš„è®¾å¤‡: {device_preference}")
                return device_preference

    def build_prompt(self, q):
        qtype = q['type']
        if qtype == 'choice':
            choices = q.get('choices', {})
            return self.prompt_templates['choice'].format(
                Question=q['prompt'],
                A=choices.get('A', ''),
                B=choices.get('B', ''),
                C=choices.get('C', ''),
                D=choices.get('D', ''),
            )
        elif qtype in self.prompt_templates:
            return self.prompt_templates[qtype].format(Question=q['prompt'])
        else:
            # å…œåº•
            return q['prompt']

    def clean_content(self, text):
        # ç§»é™¤thinkæ ‡ç­¾å‰çš„å¤šä½™å­—ç¬¦
        match = re.search(r'<think>', text)
        if match:
            text = text[match.start():]
        return text.strip()
    
    # å¤„ç†ç”Ÿæˆçš„æ–‡æœ¬
    def process_generated_texts(self, generated_texts, question_type):
        if question_type == "code-generate":
            # å¯¹äºä»£ç ç”Ÿæˆï¼Œè¿”å›æ‰€æœ‰3ä¸ªå€™é€‰ç­”æ¡ˆ
            return [self.clean_content(text) for text in generated_texts]
        else:
            # å¯¹äºå…¶ä»–ç±»å‹ï¼Œè¿”å›ç¬¬ä¸€ä¸ªç­”æ¡ˆ
            return self.clean_content(generated_texts[0]) if generated_texts else ""

    # æŒ‰ç±»å‹æ‰¹é‡å¤„ç†é¢˜ç›®ï¼Œæ¯å¤„ç†å®Œä¸€ä¸ªé¢˜ç›®å°±ç«‹å³ä¿å­˜
    def process_batch_by_type(self, questions_by_type, all_results):
        
        for qtype, questions in questions_by_type.items():
            if not questions:
                continue
            
            question_count = len(questions)
            
            # è®°å½•æ‰¹æ¬¡å¼€å§‹
            batch_start_time = self.logger.log_batch_start(qtype, question_count)
            
            # é€ä¸ªå¤„ç†é¢˜ç›®ï¼ˆæ”¯æŒå®æ—¶ä¿å­˜ï¼‰
            generation_start = time.time()
            
            for i, q_data in enumerate(questions):
                print(f"  ğŸ“ æ­£åœ¨å¤„ç†ç¬¬ {i+1}/{len(questions)} ä¸ª {qtype} ç±»å‹é¢˜ç›® (ID: {q_data['question']['id']})...")
                
                # æ„å»ºprompt
                prompt = self.build_prompt(q_data['question'])
                
                try:
                    inputs = self.tokenizer(prompt, return_tensors="pt", truncation=True, max_length=2048)
                    input_length = inputs.input_ids.shape[1]
                    print(f"    ğŸ”¤ è¾“å…¥é•¿åº¦: {input_length} tokens")
                    
                    # å°†è¾“å…¥æ•°æ®ç§»åŠ¨åˆ°æŒ‡å®šè®¾å¤‡
                    inputs = {k: v.to(self.device) for k, v in inputs.items()}
                    
                    print(f"    ğŸš€ å¼€å§‹ç”Ÿæˆ...")
                    generation_start_single = time.time()
                    
                    with torch.no_grad():
                        outputs = self.model.generate(
                            inputs['input_ids'],
                            attention_mask=inputs['attention_mask'],
                            **self.generation_params[qtype]
                        )
                    
                    generation_time_single = time.time() - generation_start_single
                    print(f"    âœ… ç”Ÿæˆå®Œæˆï¼Œè€—æ—¶: {generation_time_single:.2f}s")
                    
                    # è§£ç ç”Ÿæˆçš„æ–‡æœ¬
                    if qtype == "code-generate":
                        # å¯¹äºä»£ç ç”Ÿæˆï¼Œè¿”å›å¤šä¸ªå€™é€‰ç­”æ¡ˆ
                        generated_texts = []
                        for output in outputs:
                            # åªè§£ç æ–°ç”Ÿæˆçš„tokens
                            generated_tokens = output[input_length:]
                            generated_text = self.tokenizer.decode(generated_tokens, skip_special_tokens=True)
                            generated_texts.append(generated_text.strip())
                    else:
                        # å¯¹äºå…¶ä»–ç±»å‹ï¼Œè¿”å›å•ä¸ªç­”æ¡ˆ
                        generated_tokens = outputs[0][input_length:]
                        generated_text = self.tokenizer.decode(generated_tokens, skip_special_tokens=True)
                        generated_texts = [generated_text.strip()]
                    
                except Exception as e:
                    print(f"âš ï¸  ç”Ÿæˆç¬¬ {i+1} ä¸ªpromptæ—¶å‡ºé”™: {e}")
                    # ä½¿ç”¨é»˜è®¤é”™è¯¯æ¶ˆæ¯ä½œä¸ºåå¤‡
                    if qtype == "code-generate":
                        generated_texts = ["# ç”Ÿæˆå¤±è´¥", "# ç”Ÿæˆå¤±è´¥", "# ç”Ÿæˆå¤±è´¥"]
                    else:
                        generated_texts = ["ç”Ÿæˆå¤±è´¥ï¼Œè¯·æ£€æŸ¥è¾“å…¥"]
                
                # å¤„ç†ç»“æœ
                processed_content = self.process_generated_texts(generated_texts, qtype)
                
                # åˆ›å»ºç»“æœå¯¹è±¡
                new_result = {
                    "id": q_data['question']['id'], 
                    "content": processed_content
                }
                
                # ğŸ”¥ ç«‹å³ä¿å­˜è¿™ä¸ªç»“æœåˆ°JSONæ–‡ä»¶
                all_results = self.logger.save_single_result(new_result, all_results)
                
                # è®°å½•å•ä¸ªé—®é¢˜å®Œæˆ
                has_multiple = isinstance(processed_content, list)
                self.logger.log_question_complete(
                    q_data['question']['id'], qtype, has_multiple
                )
                
                # æ¸…ç†å†…å­˜ï¼ˆé’ˆå¯¹NPUå’ŒGPUï¼‰
                if self.device.startswith("npu"):
                    try:
                        torch.npu.empty_cache()
                    except:
                        pass
                elif self.device.startswith("cuda"):
                    try:
                        torch.cuda.empty_cache()
                    except:
                        pass
            
            generation_time = time.time() - generation_start
            
            # è®°å½•æ‰¹æ¬¡ç»“æŸ
            self.logger.log_batch_end(qtype, question_count, batch_start_time, generation_time)
        
        return all_results


    def infer(self):
        try:
            # å¼€å§‹è®¡æ—¶
            self.logger.start_timing()
            
            # æ¯æ¬¡éƒ½é‡æ–°å¼€å§‹ï¼Œä¸è¿›è¡Œæ–­ç‚¹ç»­ä¼ 
            results = []
            
            # åŠ è½½å…¨éƒ¨é¢˜ç›®
            with open(self.data_path, "r", encoding="utf-8") as fin:
                all_lines = [json.loads(line) for line in fin]
            
            # æŒ‰ç±»å‹åˆ†ç»„æ‰€æœ‰é¢˜ç›®
            questions_by_type = {
                "choice": [],
                "code-generate": [],
                "generic-generate": [],
                "math": []
            }
            
            for idx, q in enumerate(all_lines, 1):
                qtype = q.get('type', 'generic-generate')
                if qtype in questions_by_type:
                    # æµ‹è¯•æ¨¡å¼ï¼šæ¯ç§ç±»å‹åªå–ç¬¬ä¸€é“é¢˜
                    if self.test_mode and len(questions_by_type[qtype]) >= 1:
                        continue
                    
                    questions_by_type[qtype].append({
                        'question': q,
                        'idx': idx
                    })
            
            # ç»Ÿè®¡å¾…å¤„ç†é¢˜ç›®
            total_pending = sum(len(questions) for questions in questions_by_type.values())
            
            if self.test_mode:
                self.logger.logger.info("ğŸ§ª æµ‹è¯•æ¨¡å¼ï¼šæ¯ç§ç±»å‹åªå¤„ç†ç¬¬ä¸€é“é¢˜ç›®")
            
            self.logger.logger.info(f"å¼€å§‹å¤„ç† {total_pending} é“é¢˜ç›®...")
            self.logger.logger.info(f"è¾“å‡ºæ–‡ä»¶: {self.logger.output_path}")
            print(f"ğŸ’¾ å°†å®æ—¶ä¿å­˜ç»“æœåˆ°: {self.logger.output_path}")
            
            # æŒ‰ç±»å‹æ‰¹é‡å¤„ç†ï¼ˆä¼šå®æ—¶ä¿å­˜æ¯ä¸ªç»“æœï¼‰
            results = self.process_batch_by_type(questions_by_type, results)
            
            # æœ€ç»ˆç¡®è®¤ä¿å­˜å®Œæ•´ç»“æœï¼ˆæ‰€æœ‰ç»“æœå·²å®æ—¶ä¿å­˜ï¼‰
            final_result = {"result": {"results": results}}
            print(f"âœ… æ‰€æœ‰ç»“æœå·²å®æ—¶ä¿å­˜å®Œæˆï¼Œæ–‡ä»¶: {self.logger.output_path}")
            self.logger.save_results(results)
            
            # è®°å½•æ€§èƒ½ç»Ÿè®¡
            total_questions = len(results)
            self.logger.log_performance_summary(total_questions)
            
            self.logger.logger.info(f"å…¨éƒ¨å¤„ç†å®Œæ¯•ï¼Œå…±å¤„ç† {total_questions} ä¸ªé¢˜ç›®")
            
            return final_result
            
        except Exception as e:
            self.logger.log_error(f"å¤„ç†è¿‡ç¨‹ä¸­å‘ç”Ÿé”™è¯¯: {e}")
            raise
        finally:
            # ç¡®ä¿æ—¥å¿—å™¨æ­£ç¡®å…³é—­
            self.logger.close()

    # ææ„å‡½æ•°ï¼Œç¡®ä¿èµ„æºé‡Šæ”¾
    def __del__(self):
        if hasattr(self, 'logger'):
            self.logger.close()

# è§£æå‘½ä»¤è¡Œå‚æ•°
def parse_args():
    parser = argparse.ArgumentParser(description="åä¸ºè€ƒè¯•æ•°æ®å¤„ç†è„šæœ¬")
    
    parser.add_argument(
        "--model", 
        type=str, 
        default=None,
        help="æ¨¡å‹é”®åæˆ–å®Œæ•´è·¯å¾„ (å¦‚: qwen2.5-7b, qwen2-7b, æˆ–å®Œæ•´è·¯å¾„)"
    )
    
    parser.add_argument(
        "--data", 
        type=str, 
        default=DEFAULT_DATA_PATH,
        help=f"æ•°æ®æ–‡ä»¶è·¯å¾„ (é»˜è®¤: {DEFAULT_DATA_PATH})"
    )
    
    parser.add_argument(
        "--temperature",
        type=float,
        default=DEFAULT_GENERATION_CONFIG["temperature"],
        help=f"ç”Ÿæˆæ¸©åº¦ (é»˜è®¤: {DEFAULT_GENERATION_CONFIG['temperature']})"
    )
    
    parser.add_argument(
        "--top_p",
        type=float,
        default=DEFAULT_GENERATION_CONFIG["top_p"],
        help=f"Top-p é‡‡æ ·å‚æ•° (é»˜è®¤: {DEFAULT_GENERATION_CONFIG['top_p']})"
    )
    
    parser.add_argument(
        "--device",
        type=str,
        default="auto",
        choices=["auto", "cpu", "cuda", "npu"],
        help="æŒ‡å®šä½¿ç”¨çš„è®¾å¤‡ (é»˜è®¤: auto - è‡ªåŠ¨æ£€æµ‹)"
    )
    
    parser.add_argument(
        "--list-models",
        action="store_true",
        help="åˆ—å‡ºæ‰€æœ‰å¯ç”¨çš„æ¨¡å‹é”®å"
    )
    
    parser.add_argument(
        "--list-devices",
        action="store_true",
        help="åˆ—å‡ºæ‰€æœ‰å¯ç”¨çš„è®¾å¤‡"
    )
    
    parser.add_argument(
        "--test-mode",
        action="store_true",
        help="æµ‹è¯•æ¨¡å¼ï¼šåªå¤„ç†æ¯ç§ç±»å‹çš„ç¬¬ä¸€é“é¢˜ç›®"
    )
    
    parser.add_argument(
        "--clean-logs",
        action="store_true",
        help="æ¸…ç†æ‰€æœ‰æ—¥å¿—æ–‡ä»¶å’Œç»Ÿè®¡æ–‡ä»¶"
    )
    
    return parser.parse_args()

# åˆ—å‡ºæ‰€æœ‰å¯ç”¨çš„è®¾å¤‡
def list_available_devices():
    print("å¯ç”¨çš„è®¾å¤‡:")
    print("  cpu: CPU (æ€»æ˜¯å¯ç”¨)")
    
    # æ£€æµ‹NPU
    if hasattr(torch, 'npu') and torch.npu.is_available():
        device_count = torch.npu.device_count()
        print(f"  npu: åä¸ºæ˜‡è…¾NPU ({device_count} ä¸ªè®¾å¤‡)")
    else:
        print("  npu: åä¸ºæ˜‡è…¾NPU (ä¸å¯ç”¨)")
    
    # æ£€æµ‹GPU
    if torch.cuda.is_available():
        device_count = torch.cuda.device_count()
        print(f"  cuda: NVIDIA GPU ({device_count} ä¸ªè®¾å¤‡)")
    else:
        print("  cuda: NVIDIA GPU (ä¸å¯ç”¨)")
    
    print("  auto: è‡ªåŠ¨æ£€æµ‹ (æ¨è)")
    print()
    
    # æ˜¾ç¤ºå½“å‰æ¨èçš„è®¾å¤‡
    if hasattr(torch, 'npu') and torch.npu.is_available():
        print("ğŸš€ æ¨èä½¿ç”¨: npu (åä¸ºæ˜‡è…¾NPU)")
    elif torch.cuda.is_available():
        print("ğŸš€ æ¨èä½¿ç”¨: cuda (NVIDIA GPU)")
    else:
        print("ğŸš€ æ¨èä½¿ç”¨: cpu")

def main():
    # è§£æå‘½ä»¤è¡Œå‚æ•°
    args = parse_args()
    
    # å¦‚æœç”¨æˆ·è¦æ±‚åˆ—å‡ºæ¨¡å‹ï¼Œåˆ™æ˜¾ç¤ºåé€€å‡º
    if args.list_models:
        list_available_models()
        return
    
    # å¦‚æœç”¨æˆ·è¦æ±‚åˆ—å‡ºè®¾å¤‡ï¼Œåˆ™æ˜¾ç¤ºåé€€å‡º
    if args.list_devices:
        list_available_devices()
        return
    
    # å¦‚æœç”¨æˆ·è¦æ±‚æ¸…ç†æ—¥å¿—æ–‡ä»¶ï¼Œåˆ™æ‰§è¡Œæ¸…ç†åé€€å‡º
    if args.clean_logs:
        clean_log_files()
        return
    
    # è·å–å®é™…çš„æ¨¡å‹è·¯å¾„
    model_path = get_model_path(args.model)
    
    print(f"ğŸ¤– ä½¿ç”¨æ¨¡å‹: {model_path}")
    print(f"ğŸ“ æ•°æ®æ–‡ä»¶: {args.data}")
    print(f"ğŸŒ¡ï¸  æ¸©åº¦å‚æ•°: {args.temperature}")
    print(f"ğŸ¯ Top-på‚æ•°: {args.top_p}")
    print(f"ğŸ”§ æŒ‡å®šè®¾å¤‡: {args.device}")
    if args.test_mode:
        print(f"ğŸ§ª æµ‹è¯•æ¨¡å¼: å¯ç”¨")
    print("="*50)
    
    # åˆ›å»ºä¸»ç¨‹åºå®ä¾‹
    main_instance = Main(
        model_path=model_path,
        data_path=args.data,
        device=args.device,
        test_mode=args.test_mode
    )
    
    # å¦‚æœéœ€è¦ï¼Œå¯ä»¥åŠ¨æ€æ›´æ–°é‡‡æ ·å‚æ•°
    if args.temperature != DEFAULT_GENERATION_CONFIG["temperature"] or args.top_p != DEFAULT_GENERATION_CONFIG["top_p"]:
        print(f"ğŸ“ æ›´æ–°é‡‡æ ·å‚æ•°: temperature={args.temperature}, top_p={args.top_p}")
        for qtype in main_instance.generation_params:
            main_instance.generation_params[qtype]["temperature"] = args.temperature
            main_instance.generation_params[qtype]["top_p"] = args.top_p
    
    # å¼€å§‹æ¨ç†
    main_instance.infer()

if __name__ == "__main__":
    main()
